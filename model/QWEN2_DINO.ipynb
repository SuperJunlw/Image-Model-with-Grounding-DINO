{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import requests\n",
    "import torch\n",
    "from transformers import AutoProcessor, AutoModelForImageTextToText, AutoModelForZeroShotObjectDetection\n",
    "from requests.adapters import HTTPAdapter\n",
    "from requests.packages.urllib3.util.retry import Retry\n",
    "from PIL import Image\n",
    "from qwen_vl_utils import process_vision_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = \"mps\"\n",
    "else:\n",
    "    device = \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.48, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:10<00:00,  5.28s/it]\n"
     ]
    }
   ],
   "source": [
    "# Load QWEN2.5 model, 3B parameters\n",
    "input_processor = AutoProcessor.from_pretrained(\"Qwen/Qwen2-VL-2B-Instruct\")\n",
    "input_model = AutoModelForImageTextToText.from_pretrained(\"Qwen/Qwen2-VL-2B-Instruct\").to(device)\n",
    "\n",
    "dino_processor = AutoProcessor.from_pretrained(\"IDEA-Research/grounding-dino-tiny\")\n",
    "dino_model = AutoModelForZeroShotObjectDetection.from_pretrained(\"IDEA-Research/grounding-dino-tiny\").to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_path = \"../input-data/cat.jpg\"\n",
    "image_url = \"http://images.cocodataset.org/val2017/000000039769.jpg\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Caption: cat\n",
      "Detected cat with confidence 0.786 at location [346.0, 23.85, 639.41, 372.65]\n",
      "Detected cat with confidence 0.829 at location [9.57, 54.0, 316.34, 474.77]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/6m/288j1hf95f11bm2jzx6bh6d40000gn/T/ipykernel_1557/2411263516.py:35: FutureWarning: `box_threshold` is deprecated and will be removed in version 4.51.0 for `GroundingDinoProcessor.post_process_grounded_object_detection`. Use `threshold` instead.\n",
      "  results = dino_processor.post_process_grounded_object_detection(\n"
     ]
    }
   ],
   "source": [
    "image = Image.open(input_path)\n",
    "\n",
    "prompt = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\"type\": \"image\", \"image\": image},\n",
    "            {\"type\": \"text\", \"text\": \"What is in the image? Only give your answer.\"}\n",
    "        ]\n",
    "    }]\n",
    "\n",
    "text = input_processor.apply_chat_template(prompt, tokenize = False, add_generation_prompt = True)\n",
    "image_inputs, video_inputs = process_vision_info(prompt)\n",
    "\n",
    "inputs = input_processor(\n",
    "    text = [text],\n",
    "    images = image_inputs,\n",
    "    videos = video_inputs,\n",
    "    padding = True,\n",
    "    return_tensors = \"pt\",\n",
    ").to(device)\n",
    "\n",
    "output = input_model.generate(**inputs, max_new_tokens = 64)\n",
    "\n",
    "generated_text = input_processor.decode(output[0], skip_special_tokens=True)\n",
    "generated_caption = generated_text.split(\"\\n\")[-1]\n",
    "print(\"Caption:\", generated_caption)\n",
    "\n",
    "image = Image.open(requests.get(image_url, stream=True).raw)\n",
    "text_labels = [generated_caption]\n",
    "\n",
    "inputs = dino_processor(images=image, text=text_labels, return_tensors=\"pt\").to(device)\n",
    "with torch.no_grad():\n",
    "    outputs = dino_model(**inputs)\n",
    "results = dino_processor.post_process_grounded_object_detection(\n",
    "    outputs,\n",
    "    inputs.input_ids,\n",
    "    box_threshold=0.4,\n",
    "    text_threshold=0.3,\n",
    "    target_sizes=[image.size[::-1]]\n",
    ")\n",
    "# Retrieve the first image result\n",
    "result = results[0]\n",
    "for box, score, labels in zip(result[\"boxes\"], result[\"scores\"], result[\"labels\"]):\n",
    "    box = [round(x, 2) for x in box.tolist()]\n",
    "    print(f\"Detected {labels} with confidence {round(score.item(), 3)} at location {box}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## COCO Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def requests_retry_session(\n",
    "    retries=100,\n",
    "    backoff_factor=0.3,\n",
    "    status_forcelist=(500, 502, 504),\n",
    "    session=None,\n",
    "):\n",
    "    session = session or requests.Session()\n",
    "    retry = Retry(\n",
    "        total=retries,\n",
    "        read=retries,\n",
    "        connect=retries,\n",
    "        backoff_factor=backoff_factor,\n",
    "        status_forcelist=status_forcelist,\n",
    "    )\n",
    "    adapter = HTTPAdapter(max_retries=retry)\n",
    "    session.mount('http://', adapter)\n",
    "    session.mount('https://', adapter)\n",
    "    return session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "coco_dataset = open(\"../instances_val2017.json\")\n",
    "coco_json = json.load(coco_dataset)\n",
    "coco_images = coco_json[\"images\"]\n",
    "coco_annotations = coco_json[\"annotations\"]\n",
    "coco_categories = coco_json[\"categories\"]\n",
    "inputs_url = []\n",
    "image_url = []\n",
    "for images in coco_images:\n",
    "    image_inputs = []\n",
    "    for anno in coco_annotations:\n",
    "        if anno[\"image_id\"] == images[\"id\"]:\n",
    "            for categories in coco_categories:\n",
    "                if anno[\"category_id\"] == categories[\"id\"]:\n",
    "                    image_input = {\n",
    "                        \"input\": \"../input-data/\" + categories[\"name\"] + \".jpg\",\n",
    "                        \"category_id\": categories[\"id\"]\n",
    "                    }\n",
    "                    if image_input not in image_inputs:\n",
    "                        image_inputs.append(image_input)\n",
    "                    break\n",
    "    inputs_url.append(image_inputs)\n",
    "    image_url.append(images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5000\n",
      "5000\n"
     ]
    }
   ],
   "source": [
    "print(len(inputs_url))\n",
    "print(len(image_url))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/6m/288j1hf95f11bm2jzx6bh6d40000gn/T/ipykernel_1557/2583440516.py:38: FutureWarning: `box_threshold` is deprecated and will be removed in version 4.51.0 for `GroundingDinoProcessor.post_process_grounded_object_detection`. Use `threshold` instead.\n",
      "  results = dino_processor.post_process_grounded_object_detection(\n",
      "/Users/kevinxu/Library/Python/3.9/lib/python/site-packages/transformers/models/grounding_dino/processing_grounding_dino.py:95: FutureWarning: The key `labels` is will return integer ids in `GroundingDinoProcessor.post_process_grounded_object_detection` output since v4.51.0. Use `text_labels` instead to retrieve string object names.\n",
      "  warnings.warn(self.message, FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 out of 5000\n"
     ]
    }
   ],
   "source": [
    "full_results = []\n",
    "for x in range(len(inputs_url)):\n",
    "    for input_url in inputs_url[x]:\n",
    "        #opens the image input, and captions it\n",
    "        image = Image.open(input_url[\"input\"])\n",
    "        prompt = [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\"type\": \"image\", \"image\": image},\n",
    "                {\"type\": \"text\", \"text\": \"What is in the image? Only give your answer.\"}\n",
    "            ]\n",
    "        }]\n",
    "\n",
    "        text = input_processor.apply_chat_template(prompt, tokenize = False, add_generation_prompt = True)\n",
    "        image_inputs, video_inputs = process_vision_info(prompt)\n",
    "\n",
    "        inputs = input_processor(\n",
    "            text = [text],\n",
    "            images = image_inputs,\n",
    "            videos = video_inputs,\n",
    "            padding = True,\n",
    "            return_tensors = \"pt\",\n",
    "        ).to(device)\n",
    "\n",
    "        input_model_output = input_model.generate(**inputs, max_new_tokens = 64)\n",
    "        generated_text = input_processor.decode(input_model_output[0], skip_special_tokens=True)\n",
    "        generated_caption = generated_text.split(\"\\n\")[-1]\n",
    "\n",
    "        image = Image.open(requests_retry_session().get(image_url[x][\"coco_url\"], stream=True).raw)\n",
    "        # Uses generated caption to detect\n",
    "        text_labels = [generated_caption]\n",
    "\n",
    "        inputs = dino_processor(images=image, text=text_labels, return_tensors=\"pt\").to(device)\n",
    "        with torch.no_grad():\n",
    "            outputs = dino_model(**inputs)\n",
    "\n",
    "        results = dino_processor.post_process_grounded_object_detection(\n",
    "            outputs,\n",
    "            inputs.input_ids,\n",
    "            box_threshold=0.4,\n",
    "            text_threshold=0.3,\n",
    "            target_sizes=[image.size[::-1]]\n",
    "        )\n",
    "\n",
    "         # Retrieve the first image result\n",
    "        for result in results:\n",
    "            for box, score, labels in zip(result[\"boxes\"], result[\"scores\"], result[\"labels\"]):\n",
    "                box = [round(x, 2) for x in box.tolist()]\n",
    "                formatted_results = {\n",
    "                    \"image_id\": image_url[x][\"id\"],\n",
    "                    \"category_id\": input_url[\"category_id\"],\n",
    "                    \"bbox\": box,\n",
    "                    \"score\": round(score.item(), 3)\n",
    "                }\n",
    "                full_results.append(formatted_results)\n",
    "    print(str(x + 1) + \" out of \" + str(len(image_url)))\n",
    "full_results = json.dumps(full_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
