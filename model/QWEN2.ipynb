{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kevinxu/Library/Python/3.9/lib/python/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n",
      "/Users/kevinxu/Library/Python/3.9/lib/python/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import requests\n",
    "import torch\n",
    "from transformers import AutoProcessor, AutoModelForImageTextToText, AutoModelForZeroShotObjectDetection\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "from qwen_vl_utils import process_vision_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = \"mps\"\n",
    "else:\n",
    "    device = \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.48, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.27s/it]\n"
     ]
    }
   ],
   "source": [
    "# Load QWEN2.5 model, 3B parameters\n",
    "input_processor = AutoProcessor.from_pretrained(\"Qwen/Qwen2-VL-2B-Instruct\")\n",
    "input_model = AutoModelForImageTextToText.from_pretrained(\"Qwen/Qwen2.5-VL-3B-Instruct\").to(device)\n",
    "\n",
    "dino_processor = AutoProcessor.from_pretrained(\"IDEA-Research/grounding-dino-tiny\")\n",
    "dino_model = AutoModelForZeroShotObjectDetection.from_pretrained(\"IDEA-Research/grounding-dino-tiny\").to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_path = \"../input-data/cat.jpg\"\n",
    "image_url = \"http://images.cocodataset.org/val2017/000000039769.jpg\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Caption: Cat\n",
      "Detected cat with confidence 0.696 at location [9.68, 53.7, 316.47, 474.6]\n",
      "Detected cat with confidence 0.698 at location [345.9, 23.75, 639.17, 373.11]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/6m/288j1hf95f11bm2jzx6bh6d40000gn/T/ipykernel_4333/3294203688.py:35: FutureWarning: `box_threshold` is deprecated and will be removed in version 4.51.0 for `GroundingDinoProcessor.post_process_grounded_object_detection`. Use `threshold` instead.\n",
      "  results = dino_processor.post_process_grounded_object_detection(\n",
      "/Users/kevinxu/Library/Python/3.9/lib/python/site-packages/transformers/models/grounding_dino/processing_grounding_dino.py:95: FutureWarning: The key `labels` is will return integer ids in `GroundingDinoProcessor.post_process_grounded_object_detection` output since v4.51.0. Use `text_labels` instead to retrieve string object names.\n",
      "  warnings.warn(self.message, FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "image = Image.open(input_path)\n",
    "\n",
    "prompt = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\"type\": \"image\", \"image\": image},\n",
    "            {\"type\": \"text\", \"text\": \"What is in the image? Only give your answer.\"}\n",
    "        ]\n",
    "    }]\n",
    "\n",
    "text = input_processor.apply_chat_template(prompt, tokenize = False, add_generation_prompt = True)\n",
    "image_inputs, video_inputs = process_vision_info(prompt)\n",
    "\n",
    "inputs = input_processor(\n",
    "    text = [text],\n",
    "    images = image_inputs,\n",
    "    videos = video_inputs,\n",
    "    padding = True,\n",
    "    return_tensors = \"pt\",\n",
    ").to(device)\n",
    "\n",
    "output = input_model.generate(**inputs, max_new_tokens = 64)\n",
    "\n",
    "generated_text = input_processor.decode(output[0], skip_special_tokens=True)\n",
    "generated_caption = generated_text.split(\"\\n\")[-1]\n",
    "print(\"Caption:\", generated_caption)\n",
    "\n",
    "image = Image.open(requests.get(image_url, stream=True).raw)\n",
    "text_labels = [[generated_caption, \"null\"]]\n",
    "\n",
    "inputs = dino_processor(images=image, text=text_labels, return_tensors=\"pt\").to(device)\n",
    "with torch.no_grad():\n",
    "    outputs = dino_model(**inputs)\n",
    "results = dino_processor.post_process_grounded_object_detection(\n",
    "    outputs,\n",
    "    inputs.input_ids,\n",
    "    box_threshold=0.4,\n",
    "    text_threshold=0.3,\n",
    "    target_sizes=[image.size[::-1]]\n",
    ")\n",
    "# Retrieve the first image result\n",
    "result = results[0]\n",
    "for box, score, labels in zip(result[\"boxes\"], result[\"scores\"], result[\"labels\"]):\n",
    "    box = [round(x, 2) for x in box.tolist()]\n",
    "    print(f\"Detected {labels} with confidence {round(score.item(), 3)} at location {box}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## COCO Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def requests_retry_session(\n",
    "    retries=100,\n",
    "    backoff_factor=0.3,\n",
    "    status_forcelist=(500, 502, 504),\n",
    "    session=None,\n",
    "):\n",
    "    session = session or requests.Session()\n",
    "    retry = Retry(\n",
    "        total=retries,\n",
    "        read=retries,\n",
    "        connect=retries,\n",
    "        backoff_factor=backoff_factor,\n",
    "        status_forcelist=status_forcelist,\n",
    "    )\n",
    "    adapter = HTTPAdapter(max_retries=retry)\n",
    "    session.mount('http://', adapter)\n",
    "    session.mount('https://', adapter)\n",
    "    return session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "coco_dataset = open(\"../data/instances_val2017.json\")\n",
    "coco_json = json.load(coco_dataset)\n",
    "coco_images = coco_json[\"images\"]\n",
    "coco_annotations = coco_json[\"annotations\"]\n",
    "coco_categories = coco_json[\"categories\"]\n",
    "inputs_url = []\n",
    "image_url = []\n",
    "for images in coco_images:\n",
    "    image_inputs = []\n",
    "    for anno in coco_annotations:\n",
    "        if anno[\"image_id\"] == images[\"id\"]:\n",
    "            for categories in coco_categories:\n",
    "                if anno[\"category_id\"] == categories[\"id\"]:\n",
    "                    image_input = {\n",
    "                        \"input\": \"input-data/\" + categories[\"name\"] + \".jpg\",\n",
    "                        \"category_id\": categories[\"id\"]\n",
    "                    }\n",
    "                    if image_input not in image_inputs:\n",
    "                        image_inputs.append(image_input)\n",
    "                    break\n",
    "    inputs_url.append(image_inputs)\n",
    "    image_url.append(images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36781\n",
      "36781\n"
     ]
    }
   ],
   "source": [
    "print(len(inputs_url))\n",
    "print(len(image_url))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/36781 [00:00<?, ?it/s]/var/folders/6m/288j1hf95f11bm2jzx6bh6d40000gn/T/ipykernel_4333/3793156982.py:37: FutureWarning: `box_threshold` is deprecated and will be removed in version 4.51.0 for `GroundingDinoProcessor.post_process_grounded_object_detection`. Use `threshold` instead.\n",
      "  results = dino_processor.post_process_grounded_object_detection(\n",
      "/Users/kevinxu/Library/Python/3.9/lib/python/site-packages/transformers/models/grounding_dino/processing_grounding_dino.py:95: FutureWarning: The key `labels` is will return integer ids in `GroundingDinoProcessor.post_process_grounded_object_detection` output since v4.51.0. Use `text_labels` instead to retrieve string object names.\n",
      "  warnings.warn(self.message, FutureWarning)\n",
      "  0%|          | 15/36781 [01:16<52:11:26,  5.11s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 33\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;66;03m# Uses generated caption to detect\u001b[39;00m\n\u001b[1;32m     31\u001b[0m text_labels \u001b[38;5;241m=\u001b[39m [[generated_caption, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnull\u001b[39m\u001b[38;5;124m\"\u001b[39m]]\n\u001b[0;32m---> 33\u001b[0m inputs \u001b[38;5;241m=\u001b[39m \u001b[43mdino_processor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mimage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtext\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtext_labels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpt\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m     35\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m dino_model(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39minputs)\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/transformers/models/grounding_dino/processing_grounding_dino.py:185\u001b[0m, in \u001b[0;36mGroundingDinoProcessor.__call__\u001b[0;34m(self, images, text, audio, videos, **kwargs)\u001b[0m\n\u001b[1;32m    183\u001b[0m \u001b[38;5;66;03m# Get only text\u001b[39;00m\n\u001b[1;32m    184\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m images \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 185\u001b[0m     encoding_image_processor \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimage_processor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43moutput_kwargs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mimages_kwargs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    186\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    187\u001b[0m     encoding_image_processor \u001b[38;5;241m=\u001b[39m BatchFeature()\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/transformers/image_processing_utils.py:42\u001b[0m, in \u001b[0;36mBaseImageProcessor.__call__\u001b[0;34m(self, images, **kwargs)\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, images, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m BatchFeature:\n\u001b[1;32m     41\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Preprocess an image or a batch of images.\"\"\"\u001b[39;00m\n\u001b[0;32m---> 42\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpreprocess\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/transformers/models/grounding_dino/image_processing_grounding_dino.py:1530\u001b[0m, in \u001b[0;36mGroundingDinoImageProcessor.preprocess\u001b[0;34m(self, images, annotations, return_segmentation_masks, masks_path, do_resize, size, resample, do_rescale, rescale_factor, do_normalize, do_convert_annotations, image_mean, image_std, do_pad, format, return_tensors, data_format, input_data_format, pad_size, **kwargs)\u001b[0m\n\u001b[1;32m   1527\u001b[0m     images \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrescale(image, rescale_factor, input_data_format\u001b[38;5;241m=\u001b[39minput_data_format) \u001b[38;5;28;01mfor\u001b[39;00m image \u001b[38;5;129;01min\u001b[39;00m images]\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m do_normalize:\n\u001b[0;32m-> 1530\u001b[0m     images \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m   1531\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnormalize(image, image_mean, image_std, input_data_format\u001b[38;5;241m=\u001b[39minput_data_format) \u001b[38;5;28;01mfor\u001b[39;00m image \u001b[38;5;129;01min\u001b[39;00m images\n\u001b[1;32m   1532\u001b[0m     ]\n\u001b[1;32m   1534\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m do_convert_annotations \u001b[38;5;129;01mand\u001b[39;00m annotations \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1535\u001b[0m     annotations \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m   1536\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnormalize_annotation(annotation, get_image_size(image, input_data_format))\n\u001b[1;32m   1537\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m annotation, image \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(annotations, images)\n\u001b[1;32m   1538\u001b[0m     ]\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/transformers/models/grounding_dino/image_processing_grounding_dino.py:1531\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m   1527\u001b[0m     images \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrescale(image, rescale_factor, input_data_format\u001b[38;5;241m=\u001b[39minput_data_format) \u001b[38;5;28;01mfor\u001b[39;00m image \u001b[38;5;129;01min\u001b[39;00m images]\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m do_normalize:\n\u001b[1;32m   1530\u001b[0m     images \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m-> 1531\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnormalize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimage_mean\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimage_std\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_data_format\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_data_format\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m image \u001b[38;5;129;01min\u001b[39;00m images\n\u001b[1;32m   1532\u001b[0m     ]\n\u001b[1;32m   1534\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m do_convert_annotations \u001b[38;5;129;01mand\u001b[39;00m annotations \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1535\u001b[0m     annotations \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m   1536\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnormalize_annotation(annotation, get_image_size(image, input_data_format))\n\u001b[1;32m   1537\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m annotation, image \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(annotations, images)\n\u001b[1;32m   1538\u001b[0m     ]\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/transformers/image_processing_utils.py:112\u001b[0m, in \u001b[0;36mBaseImageProcessor.normalize\u001b[0;34m(self, image, mean, std, data_format, input_data_format, **kwargs)\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mnormalize\u001b[39m(\n\u001b[1;32m     80\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m     81\u001b[0m     image: np\u001b[38;5;241m.\u001b[39mndarray,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     86\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m     87\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m np\u001b[38;5;241m.\u001b[39mndarray:\n\u001b[1;32m     88\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     89\u001b[0m \u001b[38;5;124;03m    Normalize an image. image = (image - image_mean) / image_std.\u001b[39;00m\n\u001b[1;32m     90\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    110\u001b[0m \u001b[38;5;124;03m        `np.ndarray`: The normalized image.\u001b[39;00m\n\u001b[1;32m    111\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 112\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mnormalize\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    113\u001b[0m \u001b[43m        \u001b[49m\u001b[43mimage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmean\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmean\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstd\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata_format\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_format\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_data_format\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_data_format\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    114\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/transformers/image_transforms.py:443\u001b[0m, in \u001b[0;36mnormalize\u001b[0;34m(image, mean, std, data_format, input_data_format)\u001b[0m\n\u001b[1;32m    440\u001b[0m std \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(std, dtype\u001b[38;5;241m=\u001b[39mimage\u001b[38;5;241m.\u001b[39mdtype)\n\u001b[1;32m    442\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m input_data_format \u001b[38;5;241m==\u001b[39m ChannelDimension\u001b[38;5;241m.\u001b[39mLAST:\n\u001b[0;32m--> 443\u001b[0m     image \u001b[38;5;241m=\u001b[39m \u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmean\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mstd\u001b[49m\n\u001b[1;32m    444\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    445\u001b[0m     image \u001b[38;5;241m=\u001b[39m ((image\u001b[38;5;241m.\u001b[39mT \u001b[38;5;241m-\u001b[39m mean) \u001b[38;5;241m/\u001b[39m std)\u001b[38;5;241m.\u001b[39mT\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "full_results = []\n",
    "for x in range(len(inputs_url)):\n",
    "    for input_url in inputs_url[x]:\n",
    "        #opens the image input, and captions it\n",
    "        image = Image.open(input_url[\"input\"])\n",
    "        prompt = [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\"type\": \"image\", \"image\": image},\n",
    "                {\"type\": \"text\", \"text\": \"What is in the image? Only give your answer.\"}\n",
    "            ]\n",
    "        }]\n",
    "\n",
    "        text = input_processor.apply_chat_template(prompt, tokenize = False, add_generation_prompt = True)\n",
    "        image_inputs, video_inputs = process_vision_info(prompt)\n",
    "\n",
    "        inputs = input_processor(\n",
    "            text = [text],\n",
    "            images = image_inputs,\n",
    "            videos = video_inputs,\n",
    "            padding = True,\n",
    "            return_tensors = \"pt\",\n",
    "        ).to(device)\n",
    "\n",
    "        input_model_output = input_model.generate(**inputs, max_new_tokens = 64)\n",
    "        generated_text = input_processor.decode(input_model_output[0], skip_special_tokens=True)\n",
    "        generated_caption = generated_text.split(\"\\n\")[-1]\n",
    "\n",
    "        image = Image.open(requests_retry_session().get(image_url[x][\"coco_url\"], stream=True).raw)\n",
    "        # Uses generated caption to detect\n",
    "        text_labels = [generated_caption]\n",
    "\n",
    "        inputs = dino_processor(images=image, text=text_labels, return_tensors=\"pt\").to(device)\n",
    "        with torch.no_grad():\n",
    "            outputs = dino_model(**inputs)\n",
    "\n",
    "        results = dino_processor.post_process_grounded_object_detection(\n",
    "            outputs,\n",
    "            inputs.input_ids,\n",
    "            box_threshold=0.4,\n",
    "            text_threshold=0.3,\n",
    "            target_sizes=[image.size[::-1]]\n",
    "        )\n",
    "\n",
    "         # Retrieve the first image result\n",
    "        for result in results:\n",
    "            for box, score, labels in zip(result[\"boxes\"], result[\"scores\"], result[\"labels\"]):\n",
    "                box = [round(x, 2) for x in box.tolist()]\n",
    "                formatted_results = {\n",
    "                    \"image_id\": image_url[x][\"id\"],\n",
    "                    \"category_id\": input_url[\"category_id\"],\n",
    "                    \"bbox\": box,\n",
    "                    \"score\": round(score.item(), 3)\n",
    "                }\n",
    "                full_results.append(formatted_results)\n",
    "    print(str(x + 1) + \" out of \" + str(len(image_url)))\n",
    "full_results = json.dumps(full_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
